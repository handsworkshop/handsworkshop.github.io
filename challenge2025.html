<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">

<head>
    <meta name="keywords"
        content="hands,ECCV 2025,workshop,pose estimation,MANO,challenge,keypoint,gesture,robot,grasping,manipulation,hand tracking,motion capture">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />


    <link rel="stylesheet" href="main.css" type="text/css" />
    <link rel="stylesheet" href="font-awesome/css/font-awesome.min.css">
    <!--- <title></title> --->
    <title>HANDS Workshop</title>
    <!-- MathJax -->

    <!-- End MathJax -->
</head>

<body>
    <div id="main-container">
        <div id="header-container">
            <div id="header">
                <div id="header-icon-text-container">
                    <div id="header-text-container">
                        <nav class="style1">
                            <ul id="outer_list">
                                <li id="outer_li_year"><a id="current_year" href="#">2025<span id="arrow"></span></a>
                                    <ul id="top_list">
                                        <li id="style2"><a id="style3" href="workshop2025.html">2025</a></li>
                                        <li id="style2"><a id="style3" href="workshop2024.html">2024</a></li>
                                        <li id="style2"><a id="style3" href="workshop2023.html">2023</a></li>
                                        <li id="style2"><a id="style3" href="workshop2022.html">2022</a></li>
                                        <li id="style2"><a id="style3"
                                                href="https://sites.google.com/view/hands2019/home">2019</a>
                                        <li id="style2"><a id="style3"
                                                href="https://sites.google.com/view/hands2018">2018</a>
                                        <li id="style2"><a id="style3" href="">2017</a>
                                        <li id="style2"><a id="style3"
                                                href="https://labicvl.github.io/hand/Hands2016/#home">2016</a>
                                        <li id="style2"><a id="style3" href="">2015</a>
                                        </li>
                                    </ul>
                                </li>

                                <li id="outer_li"><a id="workshop_link" href="#">Workshop</a>
                                </li>

                                <li id="outer_li"><a id="challenge_link" href="#">Challenge</a>
                                </li>

                            </ul>
                        </nav>
                    </div>
                </div>

            </div>
            <div id="layout-content">
                <div id="text-img-container">
                    <div id="img-container">
                        <a href="https://hands-workshop.org/"><img src="./logos/hands.png" alt="HANDS"
                                width="100%" /></a>
                    </div>
                    <div id="text-container"></div>
                </div>
                <p>
                <div id="beamer">
                    <beam>
                        Observing and Understanding <b>Hands</b> in Action
                    </beam></br>
                    <beams>
                        in conjunction with ICCV 2025</br>
                    </beams>
                </div>

                <br>
                <div id="menu-container">
                    <div id="menu-item1"><a id="style7" href="#overview">Overview</a></div>
                    <div id="menu-item1"><a id="style7" href="#challenge1">MegoTrack</a></div>
                    <div id="menu-item1"><a id="style7" href="#challenge2">Grasp Motion</a></div>
                    <div id="menu-item1"><a id="style7" href="#challenge3">Tracker</a></div>
                    <div id="menu-item1"><a id="style7" href="#challenge4">ARCTIC</a></div>
                    <div id="menu-item1"><a id="style7" href="#contact">Contact</a></div>
                </div>
                <br>
                </p>

                <h1 id="overview">Overview</h1>
                <p>
                    We are very happy to invite recently introduced benchmarks, UmeTrack, GraspM3, and GigaHand, and
                    host a series of challenges.To
                    participate in the challenge, please fill the <a href="https://forms.gle/xq5DNaRw6w4xSqec6"><b>Google
                            Form</b></a> and accept the terms and conditions.</p>
                </p>
                <h4 align="center">Winners and prizes will be announced and awarded during the workshop.</h4>
                <h4 align="center">Please see <b>General Rules and Participation</b> and <b>Four Tracks</b> below for
                    more
                    details.</h4>

                <h2>Timeline</h1>

                    <table class="dataintable">
                        <tbody>
                            <tr>
                                <td><b>July 3 2025 (Opened)</b></td>
                                <td>Challenge data & website release & registration open</td>
                            </tr>
                            <tr>
                                <td><b>TBD</b></td>
                                <td>Challenges start</td>
                            </tr>
                            <tr>
                                <td><b>TBD</b></td>
                                <td>Registration close</td>
                            </tr>
                            <tr>
                                <td><b>TBD</b></td>
                                <td>Challenge submission deadline</td>
                            </tr>
                            <tr>
                                <td><b>TBD</b></td>
                                <td>Decisions to participants</td>
                            </tr>
                            <tr>
                              <td><b>October 17 2025</b></td>
                              <td>Technical Report Deadline for Challenges (Invited Teams)</td>
                            </tr>
                        </tbody>
                    </table>

                    <h2>General Rules and Participation</h1>
                        <p>We must follow the general challenge rules below and more rules can be found on each offical
                            track page.
                        </p>
                        <ul>
                            <li>
                                <p>To participate in the challenge, please fill the <a href="https://forms.gle/xq5DNaRw6w4xSqec6"><b>Google Form</b></a>
                                    and accept the
                                    terms and
                                    conditions.</p>
                                <ul>
                                    <li>
                                        <p>Please <b>DO</b> use your institution's email address. Please <b>DO NOT</b>
                                            use your personal
                                            email address such as gmail.com, qq.com and 163.com.</p>
                                    </li>
                                    <li>
                                        <p>Each team must register under only one user id/email id. The list of team
                                            members <b>can not be
                                                changed or rearranged</b> throughout the competition.</p>
                                    </li>
                                    <li>
                                        <p>Each team should use the same email for creating accounts in the evaluation
                                            server, and use the
                                            team name (in verbatim) in the evaluation servers.</p>
                                    </li>
                                    <li>
                                        <p>Each individual can only participate in one team and should provide the
                                            institution email at
                                            registration.</p>
                                    </li>
                                    <li>
                                        <p>The team name should be formal and we preserve the rights to change the team
                                            name after
                                            discussing with teams.</p>
                                    </li>
                                    <li>
                                        <p>Each team will receive a registration email after registration.</p>
                                    </li>
                                    <li>
                                        <p>Teams found to be registered under multiple IDs will be disqualified.</p>
                                    </li>
                                    <li>
                                        <p>For any special cases, please email the organizers.</p>
                                    </li>
                                </ul>
                            </li>

                            <li>
                                <p>The primary contact email is important during registration.</p>
                                <ul>
                                    <li>
                                        <p>We will contact participants via emails once we have an important update.</p>
                                    </li>
                                    <li>
                                        <p>If there are any special reasons or ambiguities that may lead to disputes,
                                            please email the
                                            organizers first for explanation or approval. Subsequent contact may result
                                            in disqualification.
                                        </p>
                                    </li>
                                </ul>
                            </li>

                            <li>
                                <p>To encourage fair competition, different tracks may include limits like overall model
                                    size, training
                                    dataset etc. Details can be found in each track page.</p>
                                <ul>
                                    <li>
                                        <p>Teams may use any publicly available and appropriately licensed data (if
                                            allowed by the track) to
                                            train their models in addition to the ones provided by the organizers. </p>
                                    </li>
                                    <li>
                                        <p>The daily and overall submission number may be limited based on tracks.</p>
                                    </li>
                                    <li>
                                        <p>The best performance of a team <b>CAN NOT</b> be hidden during the
                                            competition. Hiding the best
                                            performance may result in warning or even disqualification.</p>
                                    </li>
                                    <li>
                                        <p>Any supervised/unsupervised training on the validation/testing set is not
                                            allowed in this
                                            competition.</p>
                                    </li>
                                </ul>
                            </li>

                            <li>
                                <p>Reproducibility is the responsibility of the winning teams and we invite all teams to
                                    advertise their
                                    methods.</p>
                                <ul>
                                    <li>
                                        <p>Winning methods should provide their source code to reproduce their results
                                            under strict
                                            confidentiality rules if requested by organizers/other participants. If the
                                            organizing committee
                                            determines that the submitted code runs with errors or does not yield
                                            results comparable to those
                                            in the final leaderboard and the team is not willing to cooperate, it will
                                            be disqualified, and
                                            the winning place will go to the next team in the leaderboard. </p>
                                    </li>
                                    <li>
                                        <p>In order for participants to be eligible for competition prizes and be
                                            included in the official
                                            rankings (to be presented during the workshop and subsequent publications),
                                            information about
                                            their submission must be provided to organizers. Information may include,
                                            but not limited to,
                                            details on their method, synthetic and real data use, architecture and
                                            training details. </p>
                                    </li>
                                    <li>
                                        <p>For each submission, participants must keep the parameters of their method
                                            constant across all
                                            testing data for a given track.</p>
                                    </li>
                                    <li>
                                        <p>To be considered a valid candidate in the competition, the method has to beat
                                            the baseline by a
                                            non-trivial margin. A method is invalid if there is no significant technical
                                            changes. For example,
                                            if you simply replace a ResNet18 backbone with a ResNet101 backbone, it is
                                            not counted as a valid
                                            method. The organizers preserve all rights to determine the validity of the
                                            method. We will invite
                                            all valid teams to advertise their methods via 2-3 page technical report,
                                            and or poster
                                            presentation.</p>
                                    </li>
                                    <li>
                                        <p>Winners should provide a 2-3 page technical report, winner talk, and poster
                                            presentation during
                                            the workshop.</p>
                                    </li>
                                </ul>
                            </li>
                        </ul>



                        <h1 id="challenge1">MegoTrack</h1>
                        <p>
                            In XR applications, accurately estimating hand poses from egocentric cameras is important to
                            enable social presence and interactions with the environment. The primary
                            difficulties arise from the integration of multiple calibrated head-mounted cameras and the
                            on-device
                            personalization systems for these cameras. Yet, it also provides a new opportunity to
                            condition pose
                            estimation on a pre-calibrated hand shape to improve accuracy. This challenge is designed to
                            address
                            the unique problems mentioned above. This year the challenge includes two tracks: the hand
                            pose estimation track and the hand shape estimation track. For the hand pose estimation
                            track, participants
                            will be provided with calibrated stereo hand crop videos and MANO shape parameters. The
                            expected
                            results in this challenge are the MANO pose parameters. For the hand shape estimation track,
                            participants will be provided with calibrated stereo hand crop videos, assuming each video
                            captures a single
                            subject. The expected result is the MANO shape parameters, which will be evaluated using
                            vertex
                            errors in the neutral pose.
                        </p>


                        <div style="background-color: #e9e9e9; border: 1px solid black; text-align: center;">

                            <h4 align="center">Check out the following links to get started</h4>
                            <!-- <h4 align="center">S2DHand project page: <a href="https://github.com/ut-vision/S2DHand"><b>https://github.com/ut-vision/S2DHand</b></a></h4> -->
                            <h4 align="center">Challenge website: <a
                                    href="https://eval.ai/web/challenges/challenge-page/2333/overview"><b>https://eval.ai/web/challenges/challenge-page/2333/overview</b></a>
                            </h4>
                            <h4 align="center">Toolkit: <a
                                    href="https://github.com/facebookresearch/hand_tracking_toolkit/tree/main"><b>https://github.com/facebookresearch/hand_tracking_toolkit/tree/main</b></a>
                            </h4>

                        </div>

                        <h1 id="challenge2">Grasp Motion</h1>
                            <p>
                                Grasp motion generation for human-like multi-fingered hands has wide applications in animation, robotic grasping,
                                mixed reality interaction, etc. Therefore, we design a grasp motion generation challenge that aims at producing
                                physically plausible grasp motion trajectories conditioned on 3D input objects. The challenge is built on the
                                GraspM3 dataset. The baselines for grasp motion generation will be provided prior to the challenge.
                            </p>
                            
                            <h3>Important Dates</h3>
                            <ul>
                                <li>Submission deadline for results: October 10, 2025 (11:59PM PST)</li>
                                <li>Results will be shared during the HANDS workshop at ICCV 2025</li>
                            </ul>
                            
                            <h3>Rules</h3>
                            <ul>
                                <li>The evaluation process is conducted in Isaac Gym, and the test set objects are not visible to participants.</li>
                                <li>Participants are allowed to adjust simulation parameters, provided they clearly specify all modifications made to the environment. However, please note that altering these parameters may compromise the integrity of the evaluation setup and could result in rollout failures.</li>
                                <li>For fair comparisons, only methods trained using the datasets from this challenge are qualified for winning.</li>
                                <li>Participants may not use the objects not in the dataset for training, fine-tuning, self-supervised pretraining, or any other form of method development.</li>
                                <li>However, participants may use the objects from the Objaverse dataset or other datasets to evaluate the algorithm before submission.</li>
                                <li>Participants are required to generate grasping sequences based on randomized initial hand poses and lift the object by a certain distance.</li>
                            </ul>
                            
                            <div
                                style="background-color: #e9e9e9; border: 1px solid black; text-align: center; word-wrap: break-word; margin-top: 20px;">
                            
                                <h4 align="center">Check out the following links to get started</h4>
                                <h4 align="center">Challenge instructions: <a href="https://github.com/DexGraspMotionChallenge/DexGraspMotionChallenge2025/wiki"><b>https://github.com/DexGraspMotionChallenge/DexGraspMotionChallenge2025/wiki</b></a></h4>
                                <h4 align="center">Toolkit: <a href="https://github.com/DexGraspMotionChallenge/DexGraspMotionChallenge2025"><b>https://github.com/DexGraspMotionChallenge/DexGraspMotionChallenge2025</b></a></h4>
                                <h4 align="center">GraspM3 Dataset: <a href="https://lihaoming45.github.io/GraspM3/index.html"><b>https://lihaoming45.github.io/GraspM3/index.html</b></a></h4>
                            </div>

                        <h1 id="challenge3">Tracker </h1>
                        <p>Enabling dexterous robotic hands to perform complex operations
                            that align with human actions is of great significance. This challenge seeks algorithms that
                            transfer
                            human hand-object manipulation trajectories (e.g., bimanual pen capping) to dexterous
                            robotic hands
                            in simulation, aiming to reproduce physically plausible interactions. Specifically, given a
                            reference
                            motion capture sequence—including the 6D poses of the object(s) and hand(s), as well as the
                            rotations
                            of each finger—the algorithm should generate dexterous hand actions. Reference motions,
                            estimated via vision-based methods, may contain noise due to occlusions and other issues.
                            Participants can
                            develop their algorithms based on the open-source data from OakInk V2 and GigaHand. The
                            tasks
                            include manipulations involving: 1) Single-hand manipulation with a single object. 2)
                            Bimanual
                            manipulation with a single object. 3) Bimanual manipulation with two separate objects (e.g.,
                            pen cap
                            and body). Evaluations are based on trajectories from the private test set of the GigaHand
                            dataset,
                            which is currently withheld and will be publicly released one week before the submission
                            deadline.
                        </p>

                        <div
                            style="background-color: #e9e9e9; border: 1px solid black; text-align: center; word-wrap: break-word;">
                            <h4 align="center">Check out the following links to get started</h4>
                            <h4 align="center">Project page: <a href="#"><b>TBD</b></a>
                            </h4>
                            <h4 align="center"> Toolkit for loading and visualization <a href="#"><b>TBD</b></a>
                            </h4>
                            <h4 align="center">Challenge instructions: <a href="#"><b>TBD</b></a>
                            </h4>
                            <h4 align="center">Simulation environment startup <a href="#"><b>TBD</b></a>
                            </h4>

                        </div>



                        <h1 id="challenge4">ARCTIC</h1>


                        <p>
                            Humans interact with various objects daily, making holistic 3D capture of these interactions
                            crucial for
                            modeling human behavior. Most methods for reconstructing hand-object interactions require
                            pre-scanned 3D
                            object templates, which are impractical in real-world scenarios. Recently, HOLD (<a
                                href="https://zc-alexfan.github.io/hold">Fan et al. CVPR’24</a>) has shown promise in
                            category-agnostic
                            hand-object reconstruction but is limited to single-hand interaction.
                        </p>

                        <p>
                            Since we naturally interact with both hands, we host the bimanual category-agnostic
                            reconstruction task
                            where participants must reconstruct both hands and the object in 3D from a video clip,
                            without relying on
                            pre-scanned templates. This task is more challenging as bimanual manipulation exhibits
                            severe hand-object
                            occlusion and dynamic hand-object contact, leaving rooms for future development.
                        </p>

                        <p align="center" style="margin: 2; padding: 2;">
                            <img src="assets/arctic/sushi.gif" alt="Image" width="100%"
                                style="display: block; margin: 0 auto;" />
                        </p>

                        <p>
                            To benchmark this challenge, we adapt HOLD to two-hand manipulation settings and use 9
                            videos from <a href="https://github.com/zc-alexfan/arctic">ARCTIC dataset</a>'s rigid object
                            collection, one per object
                            (excluding small objects such as scissors and phone), and sourced from the test set for this
                            challenge.

                            You will be provided with HOLD baseline skeleton code for the ARCTIC setting, as well as
                            code to produce
                            data for our evaluation server to evaluate.
                        </p>


                        <h3>Important Notes</h3>
                        <ul>
                            <li>Submission deadline for results: October 9, 2025 (11:59PM CEST) </li>
                            <li>Results will be shared during the HANDS workshop at ICCV 2025</li>
                        </ul>

                        <h3>Rules</h3>
                        <ul>
                            <li>Participants cannot use groundtruth intrinsics, extrinsics, hand/object annotations, or
                                object
                                templates from ARCTIC.</li>
                            <li>Only use the provided pre-cropped ARCTIC images for the competition.</li>
                            <li>The test set groundtruth is hidden; submit predictions to our evaluation server for
                                assessment
                                (details coming soon).</li>
                            <li>Different hand trackers or methods to estimate object pose can be used if not trained on
                                ARCTIC data.
                            </li>
                            <li>Participants may need to submit code for rule violation checks.</li>
                            <li>The code must be reproducible by the organizers.</li>
                            <li>Reproduced results should match the reported results.</li>
                            <li>Participants may be disqualified if results cannot be reproduced by the organizers.</li>
                            <li>Methods must show non-trivial novelty; minor changes like hyperparameter tuning do not
                                count.</li>
                            <li>Methods must outperform the current SOTA (<a
                                    href="https://github.com/On-JungWoan/BIGS">BIGS</a>) by at least 5% to be
                                considered valid, avoiding small margin
                                improvements due to numerical errors.</li>
                            <li>We reserve the right to determine if a method is valid and eligible for awards.</li>
                            <li>Submit early to avoid server issues. Only the final valid submission before the deadline will be counted.</li>
                        </ul>

                        <p><b>Metric: </b>We use hand-relative chamfer distance, CD_h, (the lower the better) as the
                            main metric for
                            this competition. It is defined in the <a href="https://arxiv.org/abs/2311.18448">HOLD
                                paper</a>. For this
                            two-hand setting, we average the left and right hand CD_h metrics.</p>
                        <p><b>Support: </b>For general tips on processing and improvement on HOLD (see <a
                                href="https://github.com/zc-alexfan/hold/issues/6">here</a>). For other technical
                            questions, raise an
                            issue. Should you have any confusion regarding the ARCTIC challenge (e.g., regarding to the
                            rules above),
                            feel free to contact zicong.fan@inf.ethz.ch.</p>

                        <div
                            style="background-color: #e9e9e9; border: 1px solid black; text-align: center; word-wrap: break-word;">

                            <h4 align="center">Check out the following links to get started</h4>
                            <h4 align="center">HOLD project page: <a
                                    href="https://zc-alexfan.github.io/hold"><b>https://zc-alexfan.github.io/hold</b></a>
                            </h4>
                            <h4 align="center">HOLD code: <a
                                    href="https://github.com/zc-alexfan/hold"><b>https://github.com/zc-alexfan/hold</b></a>
                            </h4>
                            <h4 align="center">Challenge instructions: <a
                                    href="https://github.com/zc-alexfan/hold/blob/master/docs/arctic.md"><b>https://github.com/zc-alexfan/hold/blob/master/docs/arctic.md</b></a>
                            </h4>
                            <h4 align="center">Leaderboard: <a
                                    href="https://arctic-leaderboard.is.tuebingen.mpg.de/leaderboard"><b>https://arctic-leaderboard.is.tuebingen.mpg.de/leaderboard</b></a>
                            </h4>
                            <h4 align="center">ARCTIC dataset: <a
                                    href="https://arctic.is.tue.mpg.de/"><b>https://arctic.is.tue.mpg.de/</b></a></h4>

                        </div>



                        <h1 id="contact">Contact</h1>
                        <p>hands2025@googlegroups.com</p>


                        <div id="footer">
                            <p style="align-items: center;text-align: center;">

                                <a href="https://youtube.com/@handsworkshop" target="_Blank">
                                    <img id="page1" alt="" src="profiles/youtube.jpg">
                                </a>
                                <a href="https://github.com/handsworkshop" target="_Blank">
                                    <img id="page" alt="" src="profiles/github.png">
                                </a>
                            </p>
                        </div>
                        <script>
                            var isYearUpdated = false; // 标志，默认未更新年份

                            document.getElementById('outer_li_year').addEventListener('click', function (event) {
                                event.preventDefault(); // 阻止默认链接行为
                                // 获取第一个<li>标签中的年份
                                var year = document.querySelector('#outer_list > li:first-child > a').textContent.trim();
                                if (year > '2020') {
                                    // 构建新的href
                                    var newHref = 'workshop' + year + '.html';
                                    // 跳转到新的页面
                                    window.location.href = newHref;
                                }

                            });

                            document.getElementById('workshop_link').addEventListener('click', function (event) {
                                event.preventDefault(); // 阻止默认链接行为
                                if (!isYearUpdated) {
                                    var year = document.querySelector('#outer_list > li:first-child > a').textContent.trim();
                                    var newHref = 'workshop' + year + '.html';
                                    window.location.href = newHref;
                                }
                            });

                            document.getElementById('challenge_link').addEventListener('click', function (event) {
                                event.preventDefault(); // 阻止默认链接行为
                                if (!isYearUpdated) {
                                    var year = document.querySelector('#outer_list > li:first-child > a').textContent.trim();
                                    var newHref = 'challenge' + year + '.html';
                                    window.location.href = newHref;
                                }
                            });

                            // 获取所有带有id="style3"的a标签
                            var yearLinks = document.querySelectorAll('#style3');

                            yearLinks.forEach(function (link) {
                                link.addEventListener('click', function (event) {
                                    // 获取点击的年份
                                    var selectedYear = this.textContent.trim();
                                    if (selectedYear < '2020') {
                                        isYearUpdated = true;
                                        document.getElementById('current_year').textContent = selectedYear;
                                        // 设置标志为已更新年份

                                        window.location.href = link.href; // 确保使用 href 进行跳转
                                    } else {
                                        event.preventDefault(); // 阻止默认链接行为
                                        document.getElementById('current_year').textContent = selectedYear;

                                        // 设置标志为已更新年份
                                        isYearUpdated = true;
                                    }

                                });
                            });

                            var challengeLi = document.querySelector('#challenge_link');
                            challengeLi.classList.add('highlight');

                        </script>
</body>

</html>
