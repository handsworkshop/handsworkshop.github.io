<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">

<head>
  <meta name="keywords"
    content="hands,ICCV 2025,workshop,pose estimation,MANO,challenge,keypoint,gesture,robot,grasping,manipulation,hand tracking,motion capture">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />


  <link rel="stylesheet" href="main.css" type="text/css" />
  <link rel="stylesheet" href="font-awesome/css/font-awesome.min.css">
  <!--- <title></title> --->
  <title>HANDS Workshop</title>
  <!-- MathJax -->

  <!-- End MathJax -->
</head>

<body>
  <div id="main-container">
    <div id="header-container">
      <div id="header">
        <div id="header-icon-text-container">
          <div id="header-text-container">
            <nav class="style1">
              <ul id="outer_list">
                <li id="outer_li_year"><a id="current_year" href="#">2025<span id="arrow"></span></a>
                  <ul id="top_list">
                    <li id="style2"><a id="style3" href="workshop2025.html">2025</a></li>
                    <li id="style2"><a id="style3" href="workshop2024.html">2024</a></li>
                    <li id="style2"><a id="style3" href="workshop2023.html">2023</a></li>
                    <li id="style2"><a id="style3" href="workshop2022.html">2022</a></li>
                    <li id="style2"><a id="style3" href="https://sites.google.com/view/hands2019/home">2019</a>
                    <li id="style2"><a id="style3" href="https://sites.google.com/view/hands2018">2018</a>
                    <li id="style2"><a id="style3" href="">2017</a>
                    <li id="style2"><a id="style3" href="https://labicvl.github.io/hand/Hands2016/#home">2016</a>
                    <li id="style2"><a id="style3" href="">2015</a>
                    </li>
                  </ul>
                </li>

                <li id="outer_li"><a id="workshop_link" href="#">Workshop</a>
                </li>

                <li id="outer_li"><a id="challenge_link" href="#">Challenge</a>
                </li>

              </ul>
            </nav>
          </div>
        </div>

      </div>
      <div id="layout-content">
        <div id="text-img-container">
          <div id="img-container">
            <a href="https://hands-workshop.org/"><img src="./logos/hands.png" alt="HANDS" width="100%" /></a>
          </div>
          <div id="text-container"></div>
        </div>
        <p>
        <div id="beamer">
          <beam>
            Observing and Understanding <b>Hands</b> in Action
          </beam></br>
          <beams>
            in conjunction with ICCV 2025</br>
          </beams>
        </div>

        <br>
        <div id="menu-container">
          <div id="menu-item"><a id="style6" href="#overview">Overview</a></div>
          <div id="menu-item"><a id="style6" href="#schedule">Schedule</a></div>
          <div id="menu-item"><a id="style6" href="#papers">Papers</a></div>
          <div id="menu-item"><a id="style6" href="#speakers">Speakers</a></div>
          <div id="menu-item"><a id="style6" href="#organizers">Organizers</a></div>
          <div id="menu-item"><a id="style6" href="#sponsors">Sponsors</a></div>
          <div id="menu-item"><a id="style6" href="#contact">Contact</a></div>
        </div>
        <br>
        </p>

        <h1 id="overview">Overview </h1>
        <font size="5">
          Welcome to our HANDS@ICCV25.
        </font>
        </br>
        </br>
        <p>
          We are very happy to organize HANDS workshop. This year's workshop will be held at ICCV25. See you in
          Honolulu.
        </p>
        <p>
          The ninth edition of this workshop will emphasize the use of multimodal LLMs for hand-related tasks.
          Multimodal LLMs have revolutionized the perceptions of AI, and demonstrated groundbreaking contributions to
          multimodal understanding, zero-shot learning, and transfer learning. Those models can process and integrate
          information from different types of hand data (or modalities), allowing the model to better understand complex
          hand-object/-hand interaction situations by capturing richer, more diverse representations.
        </p>

        <p>
          During the workshop, we will explore multimodal LLMs for hand-related tasks through the talks of invited
          speakers, the presentation of accepted papers, and workshop challenges.
        </p>


        <h1 id="schedule">Schedule</h1>
        <p style="align-items: center;text-align: center;"><b> The workshop will be held on the afternoon of Oct. 20
            (Hawai'i time) during ICCV2025.</b></p>
        <p style="align-items: center;text-align: center;"><b> The detailed schedule is below.</b></p>

        <table class="dataintable">
          <tbody>
            <tr>
              <td><b>14:00 - 14:10</b></td>
              <td>Opening Remarks</td>
            </tr>
            <tr>
              <td><b>14:10 - 14:40</b></td>
              <td> Invited Talk: Srinath Sridhar</br>
                 <b>Bio:</b> Srinath Sridhar (<a href="srinathsridhar.com">srinathsridhar.com</a>) is the John E. Savage Assistant Professor of Computer Science at Brown University, where he leads the Interactive 3D Vision & Learning Lab (ivl.cs.brown.edu). He received his PhD at the Max Planck Institute for Informatics and was subsequently a postdoctoral researcher at Stanford. His research interests are in 3D computer vision and artificial intelligence. Specifically, his group builds foundational methods for 3D spatiotemporal (4D) visual understanding of the world including objects in it, humans in motion, and human-object interactions, with applications ranging from robotics to mixed reality. He is the recipient of an NSF CAREER award, a Google Research Scholar award, and his work received a Best Student Paper award at WACV and a Best Paper Honorable Mention at Eurographics. He spends part of his time as an Amazon Scholar and a visiting faculty at the Indian Institute of Science (IISc).
              </td>
            </tr>
            <tr>
              <td></td>
              <td> <b>Title:</b> Vision and Touch in Robot Learning and Interaction</br>
                <b>Abstract:</b> Touch, together with vision, is a fundamental sensing modality for robots. However, sensing and combining touch with vision has been hard due to hardware and algorithmic challenges. In this talk, I will discuss my group's work on visuo-tactile sensing and fusion. Specifically, I will introduce (1) GigaHands, a new large-scale 3D human hand activity dataset that provides visual and contact information for robot manipulation learning, and (2) UniTac, a new method for touch sensing that operates without any tactile sensors. We show that touch sensing does not always need cumbersome hardware, and can add significant information for better robot learning.
              </td>
            </tr>
 <!--            <tr>
              <td colspan="2">
                <div class="youtube">
                  <center>
                    <iframe width="100%" height="auto" class="elementor-video-iframe"
                      style="display: block;aspect-ratio:16/9;"
                      src="#" frameborder="0"
                      allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
                  </center>
                  </iframe>
                </div>
              </td>
            </tr> -->
            <tr>
              <td><b>14:40 - 15:10</b></td>
              <td> Invited Talk: Jihyun Lee</td>
            </tr>
            <tr>
              <td></td>
              <td> <b>Title:</b> Towards a Universal Generative Prior for Hands and Their Interactions </br>
                <b>Abstract:</b> We are witnessing remarkable progress in generative modeling, with recent diffusion- and flow-based models demonstrating powerful generative capabilities. In this talk, I will discuss our recent efforts to harness these advances to build a deep generative prior for hands and their interactions. Such priors capture the distribution of plausible hand shapes, poses, and interactions, serving as a universal regularizer for long-standing hand-related vision problems, such as monocular 3D reconstruction. By constraining the solution space to what is physically and semantically plausible, generative priors reduce the ill-posedness of these problems and are particularly effective for in-the-wild generalization, where training supervision is often noisy or insufficient — helping advance progress toward more robust and reliable real-world vision systems.
              </td>
            </tr>
<!--             <tr>
              <td colspan="2">
                <div class="youtube">
                  <center>
                    <iframe width="100%" height="auto" class="elementor-video-iframe"
                      style="display: block;aspect-ratio:16/9;"
                      src="#" frameborder="0"
                      allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
                  </center>
                  </iframe>
                </div>
              </td>
            </tr> -->
            <tr>
              <td> <b>15:10 - 15:40</b></td>
              <td> Invited Talk: Seungryul Baek</td>
            </tr>
            <tr>
              <td></td>
              <td> <b>Title:</b> TBD</br>
                <b>Abstract:</b> TBD
              </td>
            </tr>
<!--             <tr>
              <td colspan="2">
                <div class="youtube">
                  <center>
                    <iframe width="100%" height="auto" class="elementor-video-iframe"
                      style="display: block;aspect-ratio:16/9;"
                      src="#" frameborder="0"
                      allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
                  </center>
                  </iframe>
                </div>
              </td>
            </tr> -->
            <tr>
              <td><b>15:40 - 16:30</b></td>
              <td> Coffee break time & Poster </td>
            </tr>
            <tr>
              <td> <b>16:30 - 17:00</b></td>
              <td> Invited Talk: Jingya Wang</td>
            </tr>
            <tr>
              <td></td>
              <td> <b>Title:</b> TBD</br>
                <b>Abstract:</b> TBD
              </td>
            </tr>
<!--             <tr>
              <td colspan="2">
                <div class="youtube">
                  <center>
                    <iframe width="100%" height="auto" class="elementor-video-iframe"
                      style="display: block;aspect-ratio:16/9;"
                      src="#" frameborder="0"
                      allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
                  </center>
                  </iframe>
                </div>
              </td>
            </tr> -->
            <tr>
              <td><b>17:00 - 17:30</b></td>
              <td> Invited Talk: Rolandos Potamias </td>
            </tr>
            <tr>
              <td></td>
              <td> <b>Title:</b> TBD </br>
                <b>Abstract:</b> TBD
              </td>
            </tr>
            <tr>
              <td> <b>17:30 - 17:50</b></td>
              <td>
                Challenge winner talks
            </tr>
            <tr>
              <td> <b>17:50 - 18:00</b></td>
              <td> Closing Remarks</td>
            </tr>
          </tbody>
        </table>




        <h1 id="papers">Call For Papers</h1>
        <p>
          We will call for full-length submissions with published proceedings. Also, we will invite submission of 2-3
          page extended abstracts and recent hand-related papers/posters to the workshop.
        </p>

        <h2>Important Dates</h2>

        <table class="dataintable">
          <tbody>
            <tr>
              <td><b>2025/06/05</b></td>
              <td>Paper Submission Start</td>
            </tr>
            <tr>
              <td><s>2025/06/30(12:00AM UTC-0)</s><b>2025/07/01(23:59 GMT)</b></td>
              <td>Full Length Paper and its Supplementary Materials Submission Deadline</td>
            </tr>
            <td><s><b>2025/07/09(12:00AM UTC-0)</b></s></td>
            <td>Full Length Paper Author Notification</td>
            </tr>
            <tr>
              <td><s><b>2025/08/01</b></s></td>
              <td>Full Length Paper Camera-ready Submission Deadline</td>
            </tr>
            <tr>
              <td><b>2025/08/30 23:59 GMT</b></td>
              <td>Extended Abstracts & Posters Submission Deadline</td>
            </tr>
            <tr>
              <td><b>2025/09/14 23:59 GMT</b></td>
              <td>Extended Abstracts & Posters Submission Notification</td>
            </tr>
            <tr>
              <td><b>2025/10/01 23:59 GMT</b></td>
              <td>Extended Abstracts & Posters Camera-ready Deadline</td>
            </tr>
              <tr>
              <td><b>2025/10/15 23:59 GMT</b></td>
              <td>Invited Extended Abstracts & Posters & Technical Report Deadline</br> One can ask invitation via email hands2025@googlegroups.com </td>
            </tr>
          </tbody>
        </table>

        

        <!--  -->
        <h2>Topics</h2>
        We will cover all hand-related topics. The relevant topics include and not limited to:

        <ul id="topicstyle1">
          <li id="topicstyle2">Hand pose and shape estimation</li>
          <li id="topicstyle2">Hand & object interactions</li>
          <li id="topicstyle2">Hand detection/segmentation</li>
          <li id="topicstyle2">Hand gesture/action recognition</li>
          <li id="topicstyle2">4D hand tracking and motion capture</li>
          <li id="topicstyle2">Hand motion synthesis</li>
          <li id="topicstyle2">Hand modeling, rendering, generation</li>
          <li id="topicstyle2">Camera systems and annotation tools</li>
          <li id="topicstyle2">Novel algorithms and network architectures</li>
          <li id="topicstyle2">Multi-modal learning</li>
          <li id="topicstyle2">Self-/un-/weakly-supervised learning</li>
          <li id="topicstyle2">Generalization and adaptation</li>
          <li id="topicstyle2">Egocentric vision for AR/VR</li>
          <li id="topicstyle2">Robot grasping, object manipulation, Haptics</li>
        </ul>



        <h2>Submission Guidelines</h2>

        <p><b>Full Length Paper Submission Website:</b> <a
          href="https://openreview.net/group?id=thecvf.com/ICCV/2025/Workshop/HANDS">OpenReview</a></p>
        <p><b>Extended Abstracts and Posters Submission Website:</b> <a href="https://openreview.net/group?id=thecvf.com/ICCV/2025/Workshop/HANDS_Non-Proceedings">OpenReview Non-Proceedings</a></p>
        </br>

        <p>We accept <b>full length papers</b>, <b>extended abstracts</b> and <b>posters</b> in our workshop. The full
          length submissions should be anonymized and will be one-round peer reviewed. The accepted full length papers
          will be included in the ICCV25 conference proceedings, while others will only be presented in the workshop. We
          welcome papers that are accepted to the ICCV2025 main conference or previous conferences, and extended
          abstracts that in progress, to show the posters in our workshop.</p>

        <h3>Call for full length papers</h3>

        <p>Full length submissions should follow <a
            href="https://iccv.thecvf.com/Conferences/2025/CallForPapers">ICCV2025 submission policies</a> (no more than
          8 pages) and use the official ICCV 2025 template. Note that the submissions violate the double-blind policy
          or the dual-submission policy will be rejected without review.</p>

        <h3>Call for extended abstracts and posters</h3>

        <p>Extended abstracts are not subject to the ICCV rules, and can use other templates. They should be shorter
          than the equivalent of 4 pages in ICCV template format. Note that extended abstracts are not anonymized, and
          will not be peer reviewed. Accepted extended abstracts will only publish on our website, show in our workshop
          and not appear in the ICCV25 conference proceedings. Posters can be simply submitted with PDF and their brief
          information like paper title, authors and conference name. </p>

        <!-- <p><b>The CMT system will close after August 15. After that, extended abstracts and posters can still be
            submitted via emails (hands2025@googlegroups.com) until August 20.</b></p> -->



        <h1 id="speakers">Invited Speakers</h1>
        <div id="member-container">
          <div id="member">
            <img src="./profiles/2025/seungryul.jpg" class="profile-border">
            <p>
              <b><a href="https://sites.google.com/site/bsrvision00/">Seungryul Baek</a></b></br>
              UNIST
            </p>
          </div>
          <div id="member">
            <img src="./profiles/2025/jihyun.jpeg" class="profile-border">
            <p>
              <b><a href="https://jyunlee.github.io">Jihyun Lee</a></b></br>
              Meta
            </p>
          </div>
          <div id="member">
            <img src="./profiles/2025/rolandos.jpeg" class="profile-border">
            <p>
              <b><a href="https://profiles.imperial.ac.uk/r.potamias19">Rolandos Potamias</a></b></br>
              Imperial College London
            </p>
          </div>
          <div id="member">
            <img src="./profiles/2025/srinath.jpg" class="profile-border">
            <p>
              <b><a href="https://cs.brown.edu/people/ssrinath/">Srinath Sridhar</a></b></br>
              Brown University
            </p>
          </div>
          <div id="member">
            <img src="./profiles/2025/jingya.jpg" class="profile-border">
            <p>
              <b><a href="https://faculty.sist.shanghaitech.edu.cn/faculty/wangjingya/">Jingya Wang</a></b></br>
              ShanghaiTech University
            </p>
          </div>
        </div>

        <h1 id="organizers">Organizers</h1>
        <div id="member-container">
          <div id="member">
            <img src="./profiles/2025/hyung.jpg">
            <p>
              <b><a href="https://hyungjinchang.wordpress.com">Hyung Jin Chang</a></b></br>
              University of Birmingham
            </p>
          </div>
          <div id="member">
            <img src="./profiles/2025/rongyu.jpg">
            <p>
              <b><a href="https://gloryyrolg.github.io">Rongyu Chen</a></b></br>
              National University of Singapore
            </p>
          </div>
          <div id="member">
            <img src="./profiles/2025/zicong.jpg" class="profile-border">
            <p>
              <b><a href="https://zc-alexfan.github.io">Zicong Fan</a></b></br>
              ETH Zurich
            </p>
          </div>
          <div id="member">
            <img src="./profiles/2025/rao.jpg" class="profile-border">
            <p>
              <b><a href="https://freddierao.github.io/">Rao Fu</a></b></br>
              Brown University
            </p>
          </div>
          <div id="member">
            <img src="./profiles/2025/kun.jpg">
            <p>
              <b><a href="https://kunhe.github.io">Kun He</a></b></br>
              Meta Reality Labs
            </p>
          </div>
          <div id="member">
            <img src="./profiles/2025/kailin.jpg" class="profile-border">
            <p>
              <b><a href="https://kailinli.top/">Kailin Li</a></b></br>
              Shanghai AI Laboratory
            </p>
          </div>
          <div id="member">
            <img src="./profiles/2025/take.jpg">
            <p>
              <b><a href="https://tkhkaeio.github.io/">Take Ohkawa</a></b></br>
              University of Tokyo
            </p>
          </div>
          <div id="member">
            <img src="./profiles/2025/yoichi.jpg">
            <p>
              <b><a href="https://sites.google.com/ut-vision.org/ysato/home">Yoichi Sato</a></b></br>
              University of Tokyo
            </p>
          </div>
          <div id="member">
            <img src="./profiles/2025/linlin.jpg">
            <p>
              <b><a href="https://mu4yang.com">Linlin Yang</a></b></br>
              Communication University of China
            </p>
          </div>
          <div id="member">
            <img src="./profiles/2025/lixin.jpg">
            <p>
              <b><a href="https://lixiny.github.io">Lixin Yang</a></b></br>
              Shanghai Jiao Tong University
            </p>
          </div>
          <div id="member">
            <img src="./profiles/2025/angela.jpg" class="profile-border">
            <p>
              <b><a href="https://www.comp.nus.edu.sg/cs/people/ayao/">Angela
                  Yao</a></b></br>
              National University of Singapore
            </p>
          </div>
          <div id="member">
            <img src="./profiles/2025/qiye.jpg">
            <p>
              <b><a href="https://person.zju.edu.cn/en/yeqi">Qi Ye</a></b></br>
              Zhejiang University
            </p>
          </div>
          <div id="member">
            <img src="./profiles/2025/linguang.jpg">
            <p>
              <b><a href="https://lg-zhang.github.io/">Linguang Zhang</a></b></br>
              Meta Reality Labs
            </p>
          </div>
          <div id="member">
            <img src="./profiles/2025/zhongqun.jpg" class="profile-border">
            <p>
              <b><a href="https://zhongqunzhang.github.io/">Zhongqun Zhang</a></b></br>
              University of Birmingham
            </p>
          </div>
        </div>

        <h1 id="sponsors">Sponsors</h1>
        <div class="sponsors-container">
          <img class="sponsor-img" src="./profiles/2025/meshcapade.jpg">
          <img class="sponsor-img" src="./profiles/2025/jst.svg">
          <img class="sponsor-img" src="./profiles/2025/UoB_Crest_Logo_RGB_POS_Landscape.jpg">
        </div>

<!--         <h1 id="sponsors">Sponsors</h1>
        <p>
          Our workshop will be sponsored by Meshcapade, Japan Science and Technology Agency (JST).
        </p>
 -->
        <h1 id="contact">Contact</h1>
        <p>hands2025@googlegroups.com</p>


        <div id="footer">
          <p style="align-items: center;text-align: center;">

            <a href="https://youtube.com/@handsworkshop" target="_Blank">
              <img id="page1" alt="" src="profiles/youtube.jpg">
            </a>
            <a href="https://github.com/handsworkshop" target="_Blank">
              <img id="page" alt="" src="profiles/github.png">
            </a>
          </p>
        </div>
        <script>
          var isYearUpdated = false; // 标志，默认未更新年份

          document.getElementById('outer_li_year').addEventListener('click', function (event) {
            event.preventDefault(); // 阻止默认链接行为
            // 获取第一个<li>标签中的年份
            var year = document.querySelector('#outer_list > li:first-child > a').textContent.trim();
            if (year > '2020') {
              // 构建新的href
              var newHref = 'workshop' + year + '.html';
              // 跳转到新的页面
              window.location.href = newHref;
            }

          });

          document.getElementById('workshop_link').addEventListener('click', function (event) {
            event.preventDefault(); // 阻止默认链接行为
            if (!isYearUpdated) {
              var year = document.querySelector('#outer_list > li:first-child > a').textContent.trim();
              var newHref = 'workshop' + year + '.html';
              window.location.href = newHref;
            }
          });

          document.getElementById('challenge_link').addEventListener('click', function (event) {
            event.preventDefault(); // 阻止默认链接行为
            if (!isYearUpdated) {
              var year = document.querySelector('#outer_list > li:first-child > a').textContent.trim();
              var newHref = 'challenge' + year + '.html';
              window.location.href = newHref;
            }
          });

          // 获取所有带有id="style3"的a标签
          var yearLinks = document.querySelectorAll('#style3');

          yearLinks.forEach(function (link) {
            link.addEventListener('click', function (event) {
              // 获取点击的年份
              var selectedYear = this.textContent.trim();
              if (selectedYear < '2020') {
                isYearUpdated = true;
                document.getElementById('current_year').textContent = selectedYear;
                // 设置标志为已更新年份

                window.location.href = link.href; // 确保使用 href 进行跳转
              } else {
                event.preventDefault(); // 阻止默认链接行为
                document.getElementById('current_year').textContent = selectedYear;

                // 设置标志为已更新年份
                isYearUpdated = true;

                var currentPage = window.location.pathname.toLowerCase();
                if (currentPage.includes('challenge')) {
                  window.location.href = 'challenge' + selectedYear + '.html';
                } else {
                  window.location.href = 'workshop' + selectedYear + '.html';
                }
              }

            });
          });

          var currentPage = window.location.pathname.toLowerCase();
          if (currentPage.includes('challenge')) {
            var challengeLi = document.querySelector('#challenge_link');
            challengeLi.classList.add('highlight');
          } else {
            var workshopLi = document.querySelector('#workshop_link');
            workshopLi.classList.add('highlight');
          }

        </script>
</body>

</html>
